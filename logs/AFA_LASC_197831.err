/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2026-01-23 17:16:20,108 - dist_train_lasc.py - INFO: 
args: Namespace(config='configs/lasc_attn_reg.yaml', pooling='gmp', seg_detach=False, work_dir='work_dir_lasc', local_rank=0, radius=8, crop_size=512, backend='nccl', cpu=False)
2026-01-23 17:16:20,108 - dist_train_lasc.py - INFO: 
configs: {'backbone': {'config': 'mit_b1', 'stride': [4, 2, 2, 1], 'comments': 'LASC medical imaging dataset'}, 'dataset': {'root_dir': 'data/LASC/imageSlice', 'slice_split': [4400, 4884], 'num_classes': 2, 'crop_size': 512, 'resize_range': [512, 2048], 'rescale_range': [0.5, 2.0], 'ignore_index': 255}, 'work_dir': {'dir': 'work_dir_lasc', 'ckpt_dir': 'work_dir_lasc/checkpoints', 'pred_dir': 'work_dir_lasc/predictions', 'segs_dir': 'segs', 'tb_logger_dir': 'work_dir_lasc/tb_logger/2026-01-23-17-16'}, 'train': {'split': 'train', 'samples_per_gpu': 16, 'max_iters': 1182, 'cam_iters': 125, 'eval_iters': 125, 'log_iters': 200, 'num_workers': 8}, 'cam': {'bkg_score': 0.45, 'high_thre': 0.55, 'low_thre': 0.35, 'aff_thre': 0.4, 'scales': [1, 0.5, 1.5]}, 'val': {'split': 'val'}, 'optimizer': {'type': 'AdamW', 'learning_rate': 6e-05, 'betas': [0.9, 0.999], 'weight_decay': 0.01}, 'scheduler': {'warmup_iter': 1500, 'warmup_ratio': 1e-06, 'power': 1.0}}
2026-01-23 17:16:20,976 - dist_train_lasc.py - INFO: 
Network config: 
WeTr(
  (encoder): mit_b1(
    (patch_embed1): OverlapPatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): OverlapPatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): OverlapPatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): OverlapPatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath(drop_prob=0.014)
        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath(drop_prob=0.029)
        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath(drop_prob=0.043)
        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath(drop_prob=0.057)
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath(drop_prob=0.071)
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.086)
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.100)
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  )
  (dropout): Dropout2d(p=0.5, inplace=False)
  (decoder): SegFormerHead(
    (linear_c4): MLP(
      (proj): Linear(in_features=512, out_features=256, bias=True)
    )
    (linear_c3): MLP(
      (proj): Linear(in_features=320, out_features=256, bias=True)
    )
    (linear_c2): MLP(
      (proj): Linear(in_features=128, out_features=256, bias=True)
    )
    (linear_c1): MLP(
      (proj): Linear(in_features=64, out_features=256, bias=True)
    )
    (dropout): Dropout2d(p=0.1, inplace=False)
    (linear_fuse): ConvModule(
      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activate): ReLU(inplace=True)
    )
    (linear_pred): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
  )
  (attn_proj): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))
  (classifier): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
)
2026-01-23 17:16:21,155 - dist_train_lasc.py - INFO: 
Optimizer: 
PolyWarmupAdamW (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 6e-05
    maximize: False
    weight_decay: 0.01

Parameter Group 1
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0
    maximize: False
    weight_decay: 0.0

Parameter Group 2
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0006000000000000001
    maximize: False
    weight_decay: 0.01

Parameter Group 3
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0006000000000000001
    maximize: False
    weight_decay: 0.01
)
/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/autograd/graph.py:829: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [2, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:334.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2026-01-23 17:16:55,965 - dist_train_lasc.py - INFO: Validating...
  0%|                                                                       | 0/484 [00:00<?, ?it/s]  0%|                                                               | 1/484 [00:00<01:30,  5.34it/s]  2%|=                                                             | 11/484 [00:00<00:10, 45.86it/s]  5%|==>                                                           | 22/484 [00:00<00:06, 67.28it/s]  7%|====                                                          | 33/484 [00:00<00:05, 79.90it/s]  9%|=====>                                                        | 43/484 [00:00<00:05, 85.74it/s] 11%|======>                                                       | 54/484 [00:00<00:04, 90.67it/s] 13%|========                                                      | 65/484 [00:00<00:04, 94.23it/s] 16%|=========>                                                    | 76/484 [00:00<00:04, 97.55it/s] 18%|===========                                                   | 87/484 [00:01<00:04, 99.15it/s] 20%|============                                                 | 98/484 [00:01<00:03, 100.44it/s] 23%|=============>                                              | 109/484 [00:01<00:03, 101.30it/s] 25%|==============>                                             | 120/484 [00:01<00:03, 102.18it/s] 27%|================                                            | 131/484 [00:01<00:03, 102.54it/s] 29%|=================>                                          | 142/484 [00:01<00:03, 102.81it/s] 32%|==================>                                         | 153/484 [00:01<00:03, 102.78it/s] 34%|====================                                        | 164/484 [00:01<00:03, 102.90it/s] 36%|=====================>                                      | 175/484 [00:01<00:03, 102.72it/s] 38%|=======================                                     | 186/484 [00:01<00:02, 102.65it/s] 41%|========================                                    | 197/484 [00:02<00:02, 102.70it/s] 43%|=========================>                                  | 208/484 [00:02<00:02, 102.38it/s] 45%|===========================                                 | 219/484 [00:02<00:02, 102.45it/s] 48%|============================>                               | 230/484 [00:02<00:02, 102.34it/s] 50%|=============================>                              | 241/484 [00:02<00:02, 101.99it/s] 52%|===============================                             | 252/484 [00:02<00:02, 102.54it/s] 54%|================================>                           | 263/484 [00:02<00:02, 102.17it/s] 57%|=================================>                          | 274/484 [00:02<00:02, 102.45it/s] 59%|===================================                         | 285/484 [00:02<00:01, 102.19it/s] 61%|====================================>                       | 296/484 [00:03<00:01, 102.28it/s] 63%|======================================                      | 307/484 [00:03<00:01, 102.21it/s] 66%|=======================================                     | 318/484 [00:03<00:01, 102.30it/s] 68%|========================================>                   | 329/484 [00:03<00:01, 102.15it/s] 70%|==========================================                  | 340/484 [00:03<00:01, 102.07it/s] 73%|===========================================>                | 351/484 [00:03<00:01, 102.11it/s] 75%|============================================>               | 362/484 [00:03<00:01, 102.03it/s] 77%|==============================================              | 373/484 [00:03<00:01, 101.94it/s] 79%|===============================================>            | 384/484 [00:03<00:00, 102.00it/s] 82%|================================================>           | 395/484 [00:04<00:00, 102.03it/s] 84%|==================================================          | 406/484 [00:04<00:00, 101.96it/s] 86%|===================================================>        | 417/484 [00:04<00:00, 102.07it/s] 88%|=====================================================       | 428/484 [00:04<00:00, 102.23it/s] 91%|======================================================      | 439/484 [00:04<00:00, 102.26it/s] 93%|=======================================================>    | 450/484 [00:04<00:00, 101.68it/s] 95%|=========================================================   | 461/484 [00:04<00:00, 102.08it/s] 98%|==========================================================> | 472/484 [00:04<00:00, 102.28it/s]100%|===========================================================>| 483/484 [00:04<00:00, 102.47it/s]100%|=============================================================| 484/484 [00:04<00:00, 97.19it/s]
2026-01-23 17:17:01,332 - dist_train_lasc.py - INFO: === Validation Scores ===
2026-01-23 17:17:01,332 - dist_train_lasc.py - INFO: val cls score: 0.706612
2026-01-23 17:17:01,332 - dist_train_lasc.py - INFO: (seg/cam/aff scores N/A - no GT masks for LASC)
2026-01-23 17:17:01,404 - dist_train_lasc.py - INFO: New best checkpoint saved: cls_score=0.706612
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/scripts/dist_train_lasc.py", line 501, in <module>
[rank0]:     train(cfg=cfg)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/scripts/dist_train_lasc.py", line 313, in train
[rank0]:     cams, aff_mat = multi_scale_cam_with_aff_mat(wetr, inputs=inputs, scales=cfg.cam.scales)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/./utils/camutils.py", line 134, in multi_scale_cam_with_aff_mat
[rank0]:     _cam, _aff_mat = model(inputs_cat, cam_only=True)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1648, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1474, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/./wetr/model_attn_aff.py", line 67, in forward
[rank0]:     _x, _attns = self.encoder(x)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/./wetr/mix_transformer.py", line 375, in forward
[rank0]:     x, attns = self.forward_features(x)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/./wetr/mix_transformer.py", line 366, in forward_features
[rank0]:     x, attn = blk(x, H, W)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/./wetr/mix_transformer.py", line 169, in forward
[rank0]:     _x, _attn = self.attn(self.norm1(x), H, W)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/./wetr/mix_transformer.py", line 112, in forward
[rank0]:     attn = (attn_ * self.scale).softmax(dim=-1)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.06 GiB. GPU 0 has a total capacity of 39.49 GiB of which 4.86 GiB is free. Including non-PyTorch memory, this process has 34.62 GiB memory in use. Of the allocated memory 30.58 GiB is allocated by PyTorch, and 3.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W123 17:17:45.465818153 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
E0123 17:17:47.122009 1201803 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1201832) of binary: /mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/bin/python3
Traceback (most recent call last):
  File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mmfs1/data/group/pgh004/carrow/repo/medical_afa/.venv/lib64/python3.9/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/dist_train_lasc.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-23_17:17:47
  host      : k177.hpc.uwa.edu.au
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1201832)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
